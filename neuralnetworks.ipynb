{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQh9BzkG6zJv",
        "outputId": "3f522836-a016-44b1-a9da-660f01fc7f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Neuron Output: 26.8\n",
            "Final Output (Buy Probability): 112.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "inputs = np.array([ 2 , 5 ])\n",
        "\n",
        "\n",
        "weights_hidden = np.array([ 3 , 4 ])\n",
        "bias_hidden = 0.8\n",
        "\n",
        "\n",
        "hidden_output = np.dot(inputs, weights_hidden ) + bias_hidden\n",
        "print(\"Hidden Neuron Output:\", round(hidden_output, 2))\n",
        "\n",
        "\n",
        "weights_output = np.array([ 4])\n",
        "bias_output = 5\n",
        "\n",
        "\n",
        "output = np.dot(np.array([hidden_output]), weights_output) + bias_output\n",
        "print(\"Final Output (Buy Probability):\", round(output, 2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "inputs = np.array([5, 7])\n",
        "\n",
        "\n",
        "weights = np.array([0.1, 0.2])\n",
        "\n",
        "\n",
        "bias = 0.5\n",
        "\n",
        "\n",
        "hidden_neuron_output = np.dot(inputs, weights) + bias\n",
        "print(\"Hidden Neuron Output:\", round(hidden_neuron_output,2))\n",
        "\n",
        "\n",
        "\n",
        "weights_output = np.array([0.3])\n",
        "bias_output = -0.1\n",
        "\n",
        "output = np.dot(np.array([hidden_neuron_output]), weights_output) + bias_output\n",
        "print(\"Output Layer Neuron (Pass Probability):\", round(output, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6ej8id37KKs",
        "outputId": "23e75877-2fa1-44b9-8713-11b1b2fd06fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Neuron Output: 2.4\n",
            "Output Layer Neuron (Pass Probability): 0.62\n"
          ]
        }
      ]
    }
  ]
}


study_hours = 5

weight = 10
bias = 5


neuron_output = study_hours * weight + bias
import numpy as np

p = 72.5   

a = 80         

l = (p - a) ** 2
print("Prediction:", p)
print("Actual:", a)
print("Loss (MSE):", l)


activated_output = max(0, neuron_output)

print("Neuron Output (before activation):", neuron_output)
print("Activated Output (ReLU):", activated_output)



study_hours = 5
prediction = 7   
actual = 10      


weight = 2.0


lr = 0.01

gradient = -2 * study_hours * (actual - prediction)

weight = weight - lr * gradient

print("Updated Weight:", weight)
import pandas as pd
from typing import Dict

def task_1_load_and_inspect(file_path: str) -> pd.DataFrame:
    data = pd.read_csv(file_path)

    if data is not None and not data.empty:
        print("Yes")
    else:
        print("NO")

    print(data.head())
    print(data.shape)
    print(data.info())
    print(data.isnull().sum())
    print(data["class"].value_counts())

    # Compute tweet lengths (number of words per tweet)
    data["tweet_length"] = data["tweet"].apply(lambda x: len(str(x).split()))

    print("Overall tweet length stats:")
    print(data["tweet_length"].describe())

    print("\nTweet length stats per class:")
    print(data.groupby("class")["tweet_length"].describe())

    return data


def task_2_check_dataset_health(df: pd.DataFrame) -> Dict:
    return {
        "total_tweets": len(df),
        "missing_values": df.isnull().sum().to_dict()
    }


def task_3_analyze_class_distribution(df: pd.DataFrame) -> pd.Series:
    return df["class"].value_counts(normalize=False, sort=True, ascending=False, dropna=True)


def task_4_analyze_tweet_length(df: pd.DataFrame) -> Dict[str, float]:
    df["tweet_length"] = df["tweet"].apply(lambda x: len(str(x).split()))
    return {
        "overall_avg_word_count": df["tweet_length"].mean(),
        "avg_hate_word_count": df[df["class"] == 0]["tweet_length"].mean(),
        "avg_offensive_word_count": df[df["class"] == 1]["tweet_length"].mean(),
        "avg_neither_word_count": df[df["class"] == 2]["tweet_length"].mean()
    }


if __name__ == "__main__":
    try:
        FILE_PATH = 'hate_speech.csv'
        
        # Task 1
        df = task_1_load_and_inspect(FILE_PATH)
        print("Task 1: Data Loaded\n")
        
        # Task 2
        print("Task 2: Checking Dataset Health")
        health = task_2_check_dataset_health(df)
        for key, val in health.items():
            print(f"{key.replace('_', ' ').title()}: {val}")
        print("\n")
        
        # Task 3
        print("Task 3: Class Distribution")
        dist = task_3_analyze_class_distribution(df)
        print("Tweet counts per class:")
        print(dist)
        print("\n")
        
        # Task 4
        print("Task 4: Tweet Length Analysis")
        length_stats = task_4_analyze_tweet_length(df)
        print(f"Overall Average Word Count: {length_stats['overall_avg_word_count']:.2f} words")
        print(f"Average Hate-speech Word Count: {length_stats['avg_hate_word_count']:.2f} words")
        print(f"Average Offensive Word Count: {length_stats['avg_offensive_word_count']:.2f} words")
        print(f"Average Neutral Word Count: {length_stats['avg_neither_word_count']:.2f} words")
        
    except FileNotFoundError:
        print(f"Error: The file '{FILE_PATH}' was not found.")
    except Exception as e:
        print(f"An error occurred: {e}")

